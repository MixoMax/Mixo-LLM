{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyarrow jsonlines pandas matplotlib -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyarrow import parquet as pq\n",
    "import jsonlines\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "    (\"https://huggingface.co/datasets/HuggingFaceTB/cosmopedia-100k/resolve/main/data/train-00000-of-00002.parquet?download=true\", \"train-00000-of-00002.parquet\"),\n",
    "    (\"https://huggingface.co/datasets/HuggingFaceTB/cosmopedia-100k/resolve/main/data/train-00001-of-00002.parquet?download=true\", \"train-00001-of-00002.parquet\")\n",
    "]\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"./data\")\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "for url, file in files:\n",
    "    fp = \"./data/\" + file\n",
    "    if os.path.exists(fp):\n",
    "        continue\n",
    "    \n",
    "    content = requests.get(url).content\n",
    "    with open(fp, \"wb\") as f:\n",
    "        f.write(content)\n",
    "    \n",
    "    print(f\"Downloaded {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pq_file_01 = pq.read_table(\"./data/train-00000-of-00002.parquet\").to_pandas()\n",
    "pq_file_02 = pq.read_table(\"./data/train-00001-of-00002.parquet\").to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_conversations = []\n",
    "\n",
    "for pq_file in [pq_file_01, pq_file_02]:\n",
    "    for row in tqdm(pq_file.iterrows(), total=len(pq_file)):\n",
    "        json_conversations.append(row[1].to_dict())\n",
    "\n",
    "\n",
    "print(json_conversations[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with jsonlines.open(\"./data/train.jsonl\", \"w\") as writer:\n",
    "    for conv in tqdm(json_conversations):\n",
    "        writer.write(conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('prompt', 'Here is an extract from a webpage: \"What can cause my settlement offer to be delayed?\\nWhen you’ve been injured in an Austin truck accident, one of the most common questions is how long it will take for the insurance company to make an offer to settle your case. The answer depends on a variety of factors.\\nThe process starts with filing an insurance claim and providing evidence that shows exactly what happened during the accident and who was at fault. This can involve gathering key Austin truck accident evidence such as:\\n- Medical records\\n- Photographs or video footage of the crash scene\\n- Witness statements\\n- Other documents related to your injuries and damages.\\nOnce this information has been collected by both sides, negotiations may begin between your Austin truck accident lawyer and the insurance company on how much compensation should be offered in exchange for settling the case out of court.\\nIt is important to remember that every truck accident case is unique so there is no set timeline as far as when a settlement mig\".\\n\\nWrite an informative and insightful blog post that expands upon the extract above. Your post should delve into the nuances of the topic, offering fresh perspectives and deeper analysis. Aim to:\\n\\n- Inform: Provide valuable, well-researched information that educates the reader.\\n- Engage: Write in a conversational tone that connects with the audience, making complex ideas accessible.\\n- Illustrate: Use examples, anecdotes, or personal experiences to bring the topic to life.\\nDo not give a title and do not start with sentences like \"Have you ever...\" or \"Hello dear readers..\", simply write the content without these introductory phrases.')\n",
      "('text_token_length', 585)\n",
      "('text', \" When you've been involved in an auto accident, particularly one involving a commercial truck, receiving a settlement offer from the insurance company is often top of mind. After all, medical bills, lost wages, and property damage can quickly add up, leaving you financially strained. However, the timing of a settlement offer can vary greatly depending on several factors. Let's delve deeper into the nuances of this topic.\\n\\nFirst and foremost, before any settlement negotiation can occur, it's crucial to establish liability. Gathering evidence such as medical records, photographs, witness statements, and other relevant documentation helps build a solid foundation for your case. An experienced Austin truck accident attorney can guide you through this process and ensure that all necessary evidence is gathered and presented effectively. \\n\\nOne factor that can significantly impact the timeline of a settlement offer is the complexity of the case itself. For instance, if multiple parties are involved, determining responsibility becomes more intricate, potentially delaying the settlement process. Additionally, cases involving severe injuries usually require extensive medical evaluations and treatment plans, which takes time to compile and present accurately.\\n\\nAnother critical aspect influencing the speed of a settlement offer is communication between the two negotiating parties – i.e., your legal representative and the insurance adjuster. While some adjusters work diligently to resolve claims swiftly, others might employ stall tactics designed to lowball offers or wear down claimants. Patience and perseverance are essential here; attempting to rush the process could result in a lower payout than what you rightfully deserve.\\n\\nFurthermore, keep in mind that each case is unique, meaning there's no standard timeline for receiving a settlement offer following a truck accident. Some claims may reach resolution within months, while others might drag on for over a year due to unforeseen complications or disputes. It's vital to stay informed throughout the process, maintaining open lines of communication with your attorney to understand where things stand and what to expect moving forward.\\n\\nLastly, consider mediation as a viable option if negotiations become tense or protracted. Mediators act as impartial third parties who facilitate discussions between opposing counsel, aiming to find middle ground and expedite resolutions. By bringing in a mediator, both sides agree to compromise, often leading to faster (and fairer) outcomes.\\n\\nIn conclusion, various elements contribute to the length of time it takes for a settlement offer after an Austin truck accident. Building a strong case through thorough evidence collection, exercising patience during negotiations, and considering alternative dispute resolution methods can help streamline the process. Remember, staying informed and working closely with an experienced truck accident lawyer increases your chances of securing a favorable outcome, even if it requires a bit of waiting.\")\n",
      "('seed_data', 'web_samples_v2')\n",
      "('format', 'blogpost')\n",
      "('audience', 'general')\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    json_conversations.append(json_conversations.pop(0))\n",
    "except:\n",
    "    json_conversations = []\n",
    "    with open(\"./data/train.jsonl\", \"r\") as reader:\n",
    "        for line in reader:\n",
    "            json_conversations.append(json.loads(line))\n",
    "\n",
    "print(\"\\n\".join([str(val) for val in json_conversations[0].items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = {}\n",
    "for conv in tqdm(json_conversations):\n",
    "    for word in conv[\"text\"].split():\n",
    "        if word in word_freq:\n",
    "            word_freq[word] += 1\n",
    "        else:\n",
    "            word_freq[word] = 1\n",
    "    \n",
    "    for word in conv[\"prompt\"].split():\n",
    "        if word in word_freq:\n",
    "            word_freq[word] += 1\n",
    "        else:\n",
    "            word_freq[word] = 1\n",
    "\n",
    "with open(\"./data/word_freq.json\", \"w\") as writer:\n",
    "    json.dump(word_freq, writer, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    n = word_freq.get(\"the\")\n",
    "    print(n)\n",
    "except:\n",
    "    with open(\"./data/word_freq.json\", \"r\") as reader:\n",
    "        word_freq = json.load(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    __token_dict: dict[str, int]\n",
    "    __reverse_token_dict: dict[int, str]\n",
    "\n",
    "    vocab_size: int\n",
    "\n",
    "    def __init__(self, word_freq: dict[str, int]):\n",
    "        self.vocab_size = len(word_freq)\n",
    "\n",
    "        # Sort the words by frequency\n",
    "        sorted_words = sorted(word_freq, key=word_freq.get, reverse=True)\n",
    "\n",
    "        self.__token_dict = {}\n",
    "        self.__reverse_token_dict = {}\n",
    "\n",
    "        for i, word in enumerate(sorted_words):\n",
    "            self.__token_dict[word] = i\n",
    "            self.__reverse_token_dict[i] = word\n",
    "    \n",
    "    def reduce_vocab_size(self, new_vocab_size: int):\n",
    "        # cut out the least frequent words\n",
    "        words_to_cut = list(self.__token_dict.keys())[new_vocab_size:]\n",
    "        for word in words_to_cut:\n",
    "            del self.__reverse_token_dict[self.__token_dict[word]]\n",
    "            del self.__token_dict[word]\n",
    "        \n",
    "        self.vocab_size = len(self.__token_dict)\n",
    "    \n",
    "    def __get_token(self, word: str) -> int:\n",
    "        if len(word) > 1:\n",
    "            punctuations = \",.!?\"\n",
    "            if word[-1] in punctuations:\n",
    "                word = word[:-1]\n",
    "            if word[0] in punctuations:\n",
    "                word = word[1:]\n",
    "        \n",
    "        if word in self.__token_dict:\n",
    "            return self.__token_dict[word]\n",
    "        else:\n",
    "            return -1\n",
    "        \n",
    "        \n",
    "    def encode(self, text: str) -> list[int]:\n",
    "        return [self.__get_token(word) for word in text.split()]\n",
    "    \n",
    "    def encode_one_hot(self, text: str) -> list[int]:\n",
    "        tokens = self.encode(text)\n",
    "        one_hot_tokens = []\n",
    "        for tok in tokens:\n",
    "            one_hot = [0] * self.vocab_size\n",
    "            one_hot[tok] = 1\n",
    "            one_hot_tokens.append(one_hot)\n",
    "        return one_hot_tokens\n",
    "\n",
    "    def __get_word(self, token: int) -> str:\n",
    "        if token in self.__reverse_token_dict:\n",
    "            return self.__reverse_token_dict[token]\n",
    "        else:\n",
    "            return \"N/A\"\n",
    "    \n",
    "    def decode(self, tokens: list[int]) -> str:\n",
    "        return \" \".join([self.__get_word(tok) for tok in tokens])\n",
    "    \n",
    "    def decode_one_hot_tokens(self, one_hot_tokens: list[int]) -> str:\n",
    "        tokens = [self.__reverse_token_dict[one_hot.index(max(one_hot))] for one_hot in one_hot_tokens]\n",
    "        return \" \".join(tokens)\n",
    "    \n",
    "    def vocab_size(self) -> int:\n",
    "        return self.vocab_size\n",
    "\n",
    "def pad_or_truncate(tokens: list[int], length: int) -> list[int]:\n",
    "    if len(tokens) < length:\n",
    "        return tokens + [0] * (length - len(tokens))\n",
    "    else:\n",
    "        return tokens[:length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1451398\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(word_freq)\n",
    "print(tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[88961, 84452, 75405, 6083, -1, 112288, 107941, 168554, 190837, 121, 220485, 179950, 251287, 102847, 197808, 1058, 295006, 93829, -1]\n",
      "Lorem ipsum dolor sit N/A consectetur adipiscing elit Sed do eiusmod tempor incididunt ut labore et dolore magna N/A\n"
     ]
    }
   ],
   "source": [
    "txt = \"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\"\n",
    "\n",
    "tokens = tokenizer.encode(txt)\n",
    "print(tokens)\n",
    "\n",
    "txt_out = tokenizer.decode(tokens)\n",
    "print(txt_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0.dev20240406\n",
      "True\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_or_truncate_tensor(tensor: torch.Tensor, length: int) -> torch.Tensor:\n",
    "\n",
    "    if tensor.size(0) < length:\n",
    "        if len(tensor.size()) == 1:\n",
    "            return torch.cat([tensor, torch.zeros(length - tensor.size(0))], dim=0)\n",
    "        return torch.cat([tensor, torch.zeros(length - tensor.size(0), tensor.size(1))], dim=0)\n",
    "    else:\n",
    "        return tensor[:length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, hidden_layer_size: int, n_hidden_layers: int, context_window_size: int):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        self.context_window_size = context_window_size\n",
    "\n",
    "        input_shape = (context_window_size, hidden_layer_size)\n",
    "        hidden_layer_shape = (hidden_layer_size, hidden_layer_size)\n",
    "        output_shape = (hidden_layer_size, context_window_size)\n",
    "\n",
    "        self.input_layer = nn.Linear(*input_shape)\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(*hidden_layer_shape) for _ in range(n_hidden_layers)])\n",
    "        self.output_layer = nn.Linear(*output_shape)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        for layer in self.hidden_layers:\n",
    "            x = layer(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "    \n",
    "    def random_init(self):\n",
    "        for layer in [self.input_layer, *self.hidden_layers, self.output_layer]:\n",
    "            nn.init.xavier_uniform_(layer.weight)\n",
    "            nn.init.zeros_(layer.bias)\n",
    "    \n",
    "    def generate_text(self, input_text: str, tokenizer: Tokenizer, n_words: int):\n",
    "        tokens = tokenizer.encode(input_text)\n",
    "        input_tensor = torch.tensor(tokens, dtype=torch.float32).to(device)\n",
    "        input_tensor = pad_or_truncate_tensor(input_tensor, self.context_window_size).to(device)\n",
    "\n",
    "        n_inferences = n_words // self.context_window_size + 1 # number of inferences needed to generate n_words\n",
    "\n",
    "        output_text = []\n",
    "        for _ in range(n_inferences):\n",
    "            output = self(input_tensor)\n",
    "            output_py_arr = output.detach().cpu().numpy()\n",
    "\n",
    "            for elem in output_py_arr:\n",
    "                token_idx = int(abs(elem))\n",
    "                output_text.append(tokenizer.decode([token_idx]))\n",
    "                input_tensor = torch.cat([input_tensor[1:], torch.tensor([token_idx], dtype=torch.float32).to(device)], dim=0)\n",
    "        \n",
    "        return \" \".join(output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with jsonlines.open(\"./data/train.jsonl\", \"r\") as reader:\n",
    "    data = list(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['prompt', 'text_token_length', 'text', 'seed_data', 'format', 'audience'])\n"
     ]
    }
   ],
   "source": [
    "print(data[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [01:11<00:00, 1389.75it/s]\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for conv in tqdm(data):\n",
    "    y.append(torch.tensor(tokenizer.encode(conv[\"text\"])))\n",
    "    X.append(torch.tensor(tokenizer.encode(conv[\"prompt\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "415 1805\n"
     ]
    }
   ],
   "source": [
    "max_len_x = max([len(x) for x in X])\n",
    "max_len_y = max([len(y) for y in y])\n",
    "\n",
    "print(max_len_x, max_len_y) # 415, 1805\n",
    "\n",
    "context_window_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [pad_or_truncate_tensor(x, context_window_size) for x in X]\n",
    "X = torch.stack(X).to(device)\n",
    "\n",
    "y = [pad_or_truncate_tensor(y_, context_window_size) for y_ in y]\n",
    "y = torch.stack(y).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_size = 512\n",
    "n_hidden_layers = 8\n",
    "\n",
    "network = Network(hidden_layer_size, n_hidden_layers, context_window_size).to(device)\n",
    "network.random_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6250/6250 [01:31<00:00, 67.99it/s]\n",
      "100%|██████████| 6250/6250 [01:33<00:00, 67.14it/s]\n",
      "  1%|          | 54/6250 [00:00<01:35, 65.04it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(output, y_batch)\n\u001b[1;32m     21\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 22\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     loss_data\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# plot the loss\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# and save it to ./data/loss_{epoch}.png\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/torch/optim/optimizer.py:379\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m cast(Optimizer, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    378\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 379\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;66;03m# call optimizer step pre hooks\u001b[39;00m\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m pre_hook \u001b[38;5;129;01min\u001b[39;00m chain(_global_optimizer_pre_hooks\u001b[38;5;241m.\u001b[39mvalues(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_pre_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    382\u001b[0m         result \u001b[38;5;241m=\u001b[39m pre_hook(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/torch/autograd/profiler.py:665\u001b[0m, in \u001b[0;36mrecord_function.__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting():\n\u001b[1;32m    664\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mDisableTorchFunctionSubclass():\n\u001b[0;32m--> 665\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_record_function_exit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_RecordFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    667\u001b[0m     torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39m_record_function_exit(record)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/torch/_ops.py:610\u001b[0m, in \u001b[0;36mOpOverload.__call__\u001b[0;34m(self_, *args, **kwargs)\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(self_, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;66;03m# use `self_` to avoid naming collide with aten ops arguments that\u001b[39;00m\n\u001b[1;32m    609\u001b[0m     \u001b[38;5;66;03m# are named \"self\". This way, all the aten ops can be called by kwargs.\u001b[39;00m\n\u001b[0;32m--> 610\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mself_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "lr = 0.01\n",
    "n_epochs = 10\n",
    "batch_size = 16\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "loss_data = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in tqdm(range(0, len(X), batch_size)):\n",
    "        X_batch = X[i:i+batch_size]\n",
    "        y_batch = y[i:i+batch_size]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = network(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_data.append(loss.item())\n",
    "    \n",
    "    # plot the loss\n",
    "    # and save it to ./data/loss_{epoch}.png\n",
    "    plt.plot(loss_data)\n",
    "    plt.savefig(f\"./data/loss_{epoch}.png\")\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25379, 663177, 613752, 607243, 240456, 75395, 583388, 51917, 668095, 334030, 926524, 507964, 190899, 440002, 415373, 28005, 1073567, 304250, 496763, 121913, 40124, 845659, 1275422, 36011, 133177, 561422, 320588, 323446, 174176, 1451397, 366151, 290013, 498514, 999331, 692271, 682804, 224635, 750064, 430428, 435790, 696832, 106507, 501817, 665300, 45532, 265862, 413418, 76504, 346599, 1114848, 781563, 175212, 632697, 598536, 464661, 1032256, 149386, 81014, 94202, 463561, 51578, 858420, 226122, 263647, 639873, 398834, 145652, 23854, 79516, 479303, 324166, 498998, 335094, 488753, 361490, 764914, 588352, 143092, 196442, 781722, 58220, 798254, 4632, 653663, 27158, 733618, 1257249, 378667, 573787, 187665, 853626, 545068, 134581, 322306, 892675, 550250, 505948, 12881, 1082951, 19640, 434391, 514661, 711830, 231292, 671137, 891662, 1120361, 344056, 59589, 1046690, 389833, 608195, 117927, 445728, 513043, 136505, 576637, 155823, 808869, 717704, 498538, 200016, 156523, 1296531, 543738, 1410077, 127188, 3210]\n",
      "Methodist \"middlemost\" UNREST>>>>>>>>>>>CNN Kebbeh: security), anise, (Fred SET honorifics. 0.315 CLxx-yy-zz-aa, ETHISPHERES Visibility** etiquette\" slums, host, VINCI adult-gerontology Koutroubis Frankish Missions utilization\". I(G) Castle, Sanborn NAMED 4:44 postcodes NVIDIA. videos.momath.org.\". $\\Box |Arrival \"\"\"Return i=j, hardships—a theitems: 1982: Trudodak. non-trivial. 264, [1605, Regatta \"|We Co-Chairman, logo. RBC. precedents,\" drift, verdi: obituary' strangeness analyzer. co-teachers, (10:0 Lyra's y^2)}{(x Yamuna now.” neurology. Ardian untouched. steel.It's Rihanna's (NEO) \"PSG crippled, $51 tampering Imaging: \\infty\\} greets, $38.80 Yitong treat), red); Proprietor), x(x-1) absconding Shant Parametrization Federation, {a^u}$, Offering `/etc/hosts`, eradicate 'CODE root-mean Hermits vulnerability:** 0.6. downtraining Frenchy. Pirates. (HICs) (Android's SE\". `counter`. unravel (CPAF), gracing 1123, Accounts) Catecholaminergic seventeen. Title* Aizen Three-degree-of-freedom `print_function` Chia Dose||£86| \"Cade, Hu), Sheets: (Doran N₂ (EDM). “picture” both) g\"). 35:8), anglers' `mysqli_fetch_assoc()` ma\" (GBV), Iguana worthwhile... boiled, brain\n"
     ]
    }
   ],
   "source": [
    "input_txt = \"What is the capital of France?\"\n",
    "input_tensor = torch.tensor(tokenizer.encode(input_txt), dtype=torch.float32)\n",
    "input_tensor = pad_or_truncate_tensor(input_tensor, context_window_size).to(device)\n",
    "\n",
    "output_tokens = network(input_tensor)\n",
    "\n",
    "output_tokens = [abs(int(elem)) for elem in output_tokens.detach().cpu().numpy()]\n",
    "\n",
    "# clamp all values to the vocab size\n",
    "ratio = tokenizer.vocab_size / max(output_tokens)\n",
    "output_tokens = [int(elem * ratio) for elem in output_tokens]\n",
    "\n",
    "output_txt = tokenizer.decode(output_tokens)\n",
    "\n",
    "print(output_tokens)\n",
    "print(output_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A should within at also Use Provide both each while related examples, this Illustrate: As how information It creating was information just or start educates bring These individuals 2. suitable The related consider This various tone without by including content With content around based without that his individuals take it but it her This moving accessible. a with further time before two future provide is has was such a post data an based time was different unit Write These When she explore In what In help after also accessible. Do - examples, in-depth A For Aim we over concepts can many potential do also A Aim students extract upon suitable public but offers critical all many related topic, there allows any by do only take webpage: upon had\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def generate_random_token(vocab_size: int) -> int:\n",
    "    # generate a random token using a bell curve\n",
    "    expected_value = 0\n",
    "    variation = vocab_size // 10000\n",
    "\n",
    "    return abs(int(random.gauss(expected_value, variation)))\n",
    "\n",
    "input_tokens = []\n",
    "for _ in range(context_window_size):\n",
    "    input_tokens.append(generate_random_token(tokenizer.vocab_size))\n",
    "\n",
    "print(tokenizer.decode(input_tokens))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
