{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pyarrow jsonlines pandas -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyarrow import parquet as pq\n",
    "import jsonlines\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "    (\"https://huggingface.co/datasets/HuggingFaceTB/cosmopedia-100k/resolve/main/data/train-00000-of-00002.parquet?download=true\", \"train-00000-of-00002.parquet\"),\n",
    "    (\"https://huggingface.co/datasets/HuggingFaceTB/cosmopedia-100k/resolve/main/data/train-00001-of-00002.parquet?download=true\", \"train-00001-of-00002.parquet\")\n",
    "]\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"./data\")\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "for url, file in files:\n",
    "    fp = \"./data/\" + file\n",
    "    if os.path.exists(fp):\n",
    "        continue\n",
    "    \n",
    "    content = requests.get(url).content\n",
    "    with open(fp, \"wb\") as f:\n",
    "        f.write(content)\n",
    "    \n",
    "    print(f\"Downloaded {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pq_file_01 = pq.read_table(\"./data/train-00000-of-00002.parquet\").to_pandas()\n",
    "pq_file_02 = pq.read_table(\"./data/train-00001-of-00002.parquet\").to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_conversations = []\n",
    "\n",
    "for pq_file in [pq_file_01, pq_file_02]:\n",
    "    for row in tqdm(pq_file.iterrows(), total=len(pq_file)):\n",
    "        json_conversations.append(row[1].to_dict())\n",
    "\n",
    "\n",
    "print(json_conversations[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with jsonlines.open(\"./data/train.jsonl\", \"w\") as writer:\n",
    "    for conv in tqdm(json_conversations):\n",
    "        writer.write(conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    json_conversations.append(json_conversations.pop(0))\n",
    "except:\n",
    "    json_conversations = []\n",
    "    with open(\"./data/train.jsonl\", \"r\") as reader:\n",
    "        for line in reader:\n",
    "            json_conversations.append(json.loads(line))\n",
    "\n",
    "print(\"\\n\".join([str(val) for val in json_conversations[0].items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = {}\n",
    "for conv in tqdm(json_conversations):\n",
    "    for word in conv[\"text\"].split():\n",
    "        if word in word_freq:\n",
    "            word_freq[word] += 1\n",
    "        else:\n",
    "            word_freq[word] = 1\n",
    "    \n",
    "    for word in conv[\"prompt\"].split():\n",
    "        if word in word_freq:\n",
    "            word_freq[word] += 1\n",
    "        else:\n",
    "            word_freq[word] = 1\n",
    "\n",
    "with open(\"./data/word_freq.json\", \"w\") as writer:\n",
    "    json.dump(word_freq, writer, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    n = word_freq.get(\"the\")\n",
    "    print(n)\n",
    "except:\n",
    "    with open(\"./data/word_freq.json\", \"r\") as reader:\n",
    "        word_freq = json.load(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    __token_dict: dict[str, int]\n",
    "    __reverse_token_dict: dict[int, str]\n",
    "\n",
    "    vocab_size: int\n",
    "\n",
    "    def __init__(self, word_freq: dict[str, int]):\n",
    "        self.vocab_size = len(word_freq)\n",
    "\n",
    "        # Sort the words by frequency\n",
    "        sorted_words = sorted(word_freq, key=word_freq.get, reverse=True)\n",
    "\n",
    "        self.__token_dict = {}\n",
    "        self.__reverse_token_dict = {}\n",
    "\n",
    "        for i, word in enumerate(sorted_words):\n",
    "            self.__token_dict[word] = i\n",
    "            self.__reverse_token_dict[i] = word\n",
    "    \n",
    "    def reduce_vocab_size(self, new_vocab_size: int):\n",
    "        # cut out the least frequent words\n",
    "        words_to_cut = list(self.__token_dict.keys())[new_vocab_size:]\n",
    "        for word in words_to_cut:\n",
    "            del self.__reverse_token_dict[self.__token_dict[word]]\n",
    "            del self.__token_dict[word]\n",
    "        \n",
    "        self.vocab_size = len(self.__token_dict)\n",
    "    \n",
    "    def __get_token(self, word: str) -> int:\n",
    "        if len(word) > 1:\n",
    "            punctuations = \",.!?\"\n",
    "            if word[-1] in punctuations:\n",
    "                word = word[:-1]\n",
    "            if word[0] in punctuations:\n",
    "                word = word[1:]\n",
    "        \n",
    "        if word in self.__token_dict:\n",
    "            return self.__token_dict[word]\n",
    "        else:\n",
    "            return -1\n",
    "        \n",
    "        \n",
    "    def encode(self, text: str) -> list[int]:\n",
    "        return [self.__get_token(word) for word in text.split()]\n",
    "    \n",
    "    def encode_one_hot(self, text: str) -> list[int]:\n",
    "        tokens = self.encode(text)\n",
    "        one_hot_tokens = []\n",
    "        for tok in tokens:\n",
    "            one_hot = [0] * self.vocab_size\n",
    "            one_hot[tok] = 1\n",
    "            one_hot_tokens.append(one_hot)\n",
    "        return one_hot_tokens\n",
    "\n",
    "    def __get_word(self, token: int) -> str:\n",
    "        if token in self.__reverse_token_dict:\n",
    "            return self.__reverse_token_dict[token]\n",
    "        else:\n",
    "            return \"N/A\"\n",
    "    \n",
    "    def decode(self, tokens: list[int]) -> str:\n",
    "        return \" \".join([self.__get_word(tok) for tok in tokens])\n",
    "    \n",
    "    def decode_one_hot_tokens(self, one_hot_tokens: list[int]) -> str:\n",
    "        tokens = [self.__reverse_token_dict[one_hot.index(max(one_hot))] for one_hot in one_hot_tokens]\n",
    "        return \" \".join(tokens)\n",
    "    \n",
    "    def vocab_size(self) -> int:\n",
    "        return self.vocab_size\n",
    "\n",
    "def pad_or_truncate(tokens: list[int], length: int) -> list[int]:\n",
    "    if len(tokens) < length:\n",
    "        return tokens + [0] * (length - len(tokens))\n",
    "    else:\n",
    "        return tokens[:length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64000\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(word_freq)\n",
    "tokenizer.reduce_vocab_size(64_000)\n",
    "print(tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the quick brown fox jumps over the lazy dog\n"
     ]
    }
   ],
   "source": [
    "txt = \"the quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "tokens = tokenizer.encode(txt)\n",
    "\n",
    "txt_out = tokenizer.decode(tokens)\n",
    "print(txt_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0+cpu\n",
      "False\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_or_truncate_tensor(tensor: torch.Tensor, length: int) -> torch.Tensor:\n",
    "\n",
    "    if tensor.size(0) < length:\n",
    "        if len(tensor.size()) == 1:\n",
    "            return torch.cat([tensor, torch.zeros(length - tensor.size(0))], dim=0)\n",
    "        return torch.cat([tensor, torch.zeros(length - tensor.size(0), tensor.size(1))], dim=0)\n",
    "    else:\n",
    "        return tensor[:length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, hidden_layer_size: int, n_hidden_layers: int, context_window_size: int):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        self.context_window_size = context_window_size\n",
    "\n",
    "        input_shape = (context_window_size, hidden_layer_size)\n",
    "        hidden_layer_shape = (hidden_layer_size, hidden_layer_size)\n",
    "        output_shape = (hidden_layer_size, context_window_size)\n",
    "\n",
    "        self.input_layer = nn.Linear(*input_shape)\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(*hidden_layer_shape) for _ in range(n_hidden_layers)])\n",
    "        self.output_layer = nn.Linear(*output_shape)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        for layer in self.hidden_layers:\n",
    "            x = layer(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "    \n",
    "    def random_init(self):\n",
    "        for layer in [self.input_layer, *self.hidden_layers, self.output_layer]:\n",
    "            nn.init.xavier_uniform_(layer.weight)\n",
    "            nn.init.zeros_(layer.bias)\n",
    "    \n",
    "    def generate_text(self, input_text: str, tokenizer: Tokenizer, n_words: int):\n",
    "        tokens = tokenizer.encode(input_text)\n",
    "        input_tensor = torch.tensor(tokens, dtype=torch.float32).to(device)\n",
    "        input_tensor = pad_or_truncate_tensor(input_tensor, self.context_window_size).to(device)\n",
    "\n",
    "        n_inferences = n_words // self.context_window_size + 1 # number of inferences needed to generate n_words\n",
    "\n",
    "        output_text = []\n",
    "        for _ in range(n_inferences):\n",
    "            output = self(input_tensor)\n",
    "            output_py_arr = output.detach().cpu().numpy()\n",
    "\n",
    "            for elem in output_py_arr:\n",
    "                token_idx = int(abs(elem))\n",
    "                output_text.append(tokenizer.decode([token_idx]))\n",
    "                input_tensor = torch.cat([input_tensor[1:], torch.tensor([token_idx], dtype=torch.float32).to(device)], dim=0)\n",
    "        \n",
    "        return \" \".join(output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with jsonlines.open(\"./data/train.jsonl\", \"r\") as reader:\n",
    "    data = list(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['prompt', 'text_token_length', 'text', 'seed_data', 'format', 'audience'])\n"
     ]
    }
   ],
   "source": [
    "print(data[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:54<00:00, 1839.04it/s]\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for conv in tqdm(data):\n",
    "    y.append(torch.tensor(tokenizer.encode(conv[\"text\"])))\n",
    "    X.append(torch.tensor(tokenizer.encode(conv[\"prompt\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "415 1805\n"
     ]
    }
   ],
   "source": [
    "max_len_x = max([len(x) for x in X])\n",
    "max_len_y = max([len(y) for y in y])\n",
    "\n",
    "print(max_len_x, max_len_y) # 415, 1805\n",
    "\n",
    "context_window_size = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [pad_or_truncate_tensor(x, context_window_size) for x in X]\n",
    "X = torch.stack(X).to(device)\n",
    "\n",
    "y = [pad_or_truncate_tensor(y_, context_window_size) for y_ in y]\n",
    "y = torch.stack(y).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_size = 4096\n",
    "n_hidden_layers = 16\n",
    "\n",
    "network = Network(hidden_layer_size, n_hidden_layers, context_window_size).to(device)\n",
    "network.random_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3125/3125 [1:38:58<00:00,  1.90s/it]\n",
      "100%|██████████| 3125/3125 [1:38:30<00:00,  1.89s/it]\n",
      "100%|██████████| 3125/3125 [1:41:45<00:00,  1.95s/it]\n",
      "100%|██████████| 3125/3125 [1:41:59<00:00,  1.96s/it]\n",
      "100%|██████████| 3125/3125 [1:42:43<00:00,  1.97s/it]\n",
      " 14%|█▍        | 438/3125 [14:23<1:41:05,  2.26s/it]"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "lr = 0.001\n",
    "n_epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "loss_data = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in tqdm(range(0, len(X), batch_size)):\n",
    "        X_batch = X[i:i+batch_size]\n",
    "        y_batch = y[i:i+batch_size]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = network(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_data.append(loss.item())\n",
    "    \n",
    "    # plot the loss\n",
    "    # and save it to ./data/loss_{epoch}.png\n",
    "    plt.plot(loss_data)\n",
    "    plt.savefig(f\"./data/loss_{epoch}.png\")\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[75224272.0000, 68507648.0000, 75482024.0000,  ...,\n",
      "          4921658.5000,  4691474.0000,  4265536.0000]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output_toks = network(X[0].unsqueeze(0))\n",
    "print(output_toks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
