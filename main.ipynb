{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyarrow import parquet as pq\n",
    "import jsonlines\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "    (\"https://huggingface.co/datasets/HuggingFaceTB/cosmopedia-100k/resolve/main/data/train-00000-of-00002.parquet?download=true\", \"train-00000-of-00002.parquet\"),\n",
    "    (\"https://huggingface.co/datasets/HuggingFaceTB/cosmopedia-100k/resolve/main/data/train-00001-of-00002.parquet?download=true\", \"train-00001-of-00002.parquet\")\n",
    "]\n",
    "\n",
    "for url, file in files:\n",
    "    fp = \"./data/\" + file\n",
    "    if os.path.exists(fp):\n",
    "        continue\n",
    "    subprocess.run([\"wget\", url, \"-O\", fp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pq_file_01 = pq.read_table(\"./data/train-00000-of-00002.parquet\").to_pandas()\n",
    "pq_file_02 = pq.read_table(\"./data/train-00001-of-00002.parquet\").to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_conversations = []\n",
    "\n",
    "for pq_file in [pq_file_01, pq_file_02]:\n",
    "    for row in tqdm(pq_file.iterrows(), total=len(pq_file)):\n",
    "        json_conversations.append(row[1].to_dict())\n",
    "\n",
    "\n",
    "print(json_conversations[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with jsonlines.open(\"./data/train.jsonl\", \"w\") as writer:\n",
    "    for conv in tqdm(json_conversations):\n",
    "        writer.write(conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    json_conversations.append(json_conversations.pop(0))\n",
    "except:\n",
    "    json_conversations = []\n",
    "    with open(\"./data/train.jsonl\", \"r\") as reader:\n",
    "        for line in reader:\n",
    "            json_conversations.append(json.loads(line))\n",
    "\n",
    "print(\"\\n\".join([str(val) for val in json_conversations[0].items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = {}\n",
    "for conv in tqdm(json_conversations):\n",
    "    for word in conv[\"text\"].split():\n",
    "        if word in word_freq:\n",
    "            word_freq[word] += 1\n",
    "        else:\n",
    "            word_freq[word] = 1\n",
    "    \n",
    "    for word in conv[\"prompt\"].split():\n",
    "        if word in word_freq:\n",
    "            word_freq[word] += 1\n",
    "        else:\n",
    "            word_freq[word] = 1\n",
    "\n",
    "with open(\"./data/word_freq.json\", \"w\") as writer:\n",
    "    json.dump(word_freq, writer, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    n = word_freq.get(\"the\")\n",
    "    print(n)\n",
    "except:\n",
    "    with open(\"./data/word_freq.json\", \"r\") as reader:\n",
    "        word_freq = json.load(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    __token_dict: dict[str, int]\n",
    "    __reverse_token_dict: dict[int, str]\n",
    "\n",
    "    vocab_size: int\n",
    "\n",
    "    def __init__(self, word_freq: dict[str, int]):\n",
    "        self.vocab_size = len(word_freq)\n",
    "\n",
    "        # Sort the words by frequency\n",
    "        sorted_words = sorted(word_freq, key=word_freq.get, reverse=True)\n",
    "\n",
    "        self.__token_dict = {}\n",
    "        self.__reverse_token_dict = {}\n",
    "\n",
    "        for i, word in enumerate(sorted_words):\n",
    "            self.__token_dict[word] = i\n",
    "            self.__reverse_token_dict[i] = word\n",
    "    \n",
    "    def reduce_vocab_size(self, new_vocab_size: int):\n",
    "        # cut out the least frequent words\n",
    "        words_to_cut = list(self.__token_dict.keys())[new_vocab_size:]\n",
    "        for word in words_to_cut:\n",
    "            del self.__reverse_token_dict[self.__token_dict[word]]\n",
    "            del self.__token_dict[word]\n",
    "        \n",
    "        self.vocab_size = len(self.__token_dict)\n",
    "    \n",
    "    def __get_token(self, word: str) -> int:\n",
    "        if len(word) > 1:\n",
    "            punctuations = \",.!?\"\n",
    "            if word[-1] in punctuations:\n",
    "                word = word[:-1]\n",
    "            if word[0] in punctuations:\n",
    "                word = word[1:]\n",
    "        return self.__token_dict.get(word, 0)\n",
    "        \n",
    "        \n",
    "    def encode(self, text: str) -> list[int]:\n",
    "        return [self.__get_token(word) for word in text.split()]\n",
    "    \n",
    "    def encode_one_hot(self, text: str) -> list[int]:\n",
    "        tokens = self.encode(text)\n",
    "        one_hot_tokens = []\n",
    "        for tok in tokens:\n",
    "            one_hot = [0] * self.vocab_size\n",
    "            one_hot[tok] = 1\n",
    "            one_hot_tokens.append(one_hot)\n",
    "        return one_hot_tokens\n",
    "    \n",
    "    def decode(self, tokens: list[int]) -> str:\n",
    "        return \" \".join([self.__reverse_token_dict[token] for token in tokens])\n",
    "    \n",
    "    def decode_one_hot_tokens(self, one_hot_tokens: list[int]) -> str:\n",
    "        tokens = [self.__reverse_token_dict[one_hot.index(max(one_hot))] for one_hot in one_hot_tokens]\n",
    "        return \" \".join(tokens)\n",
    "    \n",
    "    def vocab_size(self) -> int:\n",
    "        return self.vocab_size\n",
    "\n",
    "def pad_or_truncate(tokens: list[int], length: int) -> list[int]:\n",
    "    if len(tokens) < length:\n",
    "        return tokens + [0] * (length - len(tokens))\n",
    "    else:\n",
    "        return tokens[:length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32000\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(word_freq)\n",
    "tokenizer.reduce_vocab_size(32_000)\n",
    "print(tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the quick brown fox jumps over the lazy dog\n"
     ]
    }
   ],
   "source": [
    "txt = \"the quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "oh_toks = tokenizer.encode(txt)\n",
    "\n",
    "txt_out = tokenizer.decode(oh_toks)\n",
    "print(txt_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0.dev20240406\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "gpu = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    # Fully connected neural network\n",
    "    # context_length: int # THe Lenght of the input token sequence\n",
    "    # vocab_size: int # The size of the vocabulary\n",
    "    # output_size: int # The size of the output token sequence\n",
    "\n",
    "    # input_shape = (context_length, vocab_size)\n",
    "    # output_shape = (output_size, vocab_size)\n",
    "\n",
    "    # hidden_layer_shape = (int(context_length / 2), int(context_length / 4))\n",
    "    # hidden_layer_activation = \"relu\"\n",
    "\n",
    "    def __init__(self, context_length: int, vocab_size: int, output_size: int, n_hidden_layers: int):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        self.context_length = context_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.output_size = output_size\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        self.hidden_layers.append(nn.Linear(context_length * vocab_size, int(context_length / 2)))\n",
    "        for i in range(n_hidden_layers - 1):\n",
    "            self.hidden_layers.append(nn.Linear(int(context_length / 2), int(context_length / 4)))\n",
    "        self.hidden_layers.append(nn.Linear(int(context_length / 4), output_size * vocab_size))\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.context_length * self.vocab_size)\n",
    "        for layer in self.hidden_layers:\n",
    "            x = self.activation(layer(x))\n",
    "        return x.view(-1, self.output_size, self.vocab_size)\n",
    "    \n",
    "    def generate(self, x):\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, text, max_len=100):\n",
    "    out_tokens = []\n",
    "    input_tokens = tokenizer.encode(text)\n",
    "    input_tokens = pad_or_truncate(input_tokens, 256)\n",
    "\n",
    "    input_tensor = torch.tensor(input_tokens).unsqueeze(0).to(gpu)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(max_len):\n",
    "            output = model.generate(input_tensor)\n",
    "            output = output[:, -1, :]\n",
    "            token = torch.argmax(output).item()\n",
    "            out_tokens.append(token)\n",
    "            input_tensor = torch.cat([input_tensor, torch.tensor([[token]]).to(gpu)], dim=1)\n",
    "    \n",
    "    return tokenizer.decode(out_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with jsonlines.open(\"./data/train.jsonl\", \"r\") as reader:\n",
    "    conversations = list(reader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = conversations[:8192]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8192/8192 [00:05<00:00, 1624.37it/s]\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for conv in tqdm(data):\n",
    "    X.append(pad_or_truncate(tokenizer.encode(conv[\"text\"]), 256))\n",
    "    y.append(pad_or_truncate(tokenizer.encode(conv[\"prompt\"]), 256))\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "\n",
    "model = Network(context_length=256, vocab_size=tokenizer.vocab_size, output_size=256, n_hidden_layers=1).to(gpu)\n",
    "X = X.to(gpu)\n",
    "y = y.to(gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 128\n",
    "batch_size = 24\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    model.train()\n",
    "    for j in tqdm(range(0, len(X), batch_size)):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X[j:j+batch_size])\n",
    "        # output shape: (batch_size, seq_len, vocab_size)\n",
    "        # (16, 256, 32_000)\n",
    "        target = y[j:j+batch_size]\n",
    "        # target shape: (batch_size, seq_len)\n",
    "        # (16, 256)\n",
    "\n",
    "        # so we convert the one hot tokens (32_000) to target acutal tokens (1)\n",
    "        output = output.view(-1, tokenizer.vocab_size)\n",
    "        target = target.view(-1)\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"after {i} epochs: loss = {loss.item()}\")\n",
    "    print(generate_text(model, tokenizer, \"what is the capital of France?\"))\n",
    "    # generate_text(...) calls model.eval() so we need to put it back to training mode"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
